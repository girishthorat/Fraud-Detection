{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fraud_Det_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP70oOVzfEV3UPFMN6Gu0f1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girishthorat/Fraud-Detection/blob/main/Fraud_Det_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5webHkIndnFw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import argmax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrsA7UsjeHS6"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix # evaluation metric\n",
        "from sklearn.metrics import accuracy_score # evaluation metric\n",
        "from sklearn.metrics import f1_score # evaluation metric\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split # data split\n",
        "from matplotlib import pyplot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbvsxkhCkS_0"
      },
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Dense,Activation,Dropout \n",
        "from keras.layers.normalization import BatchNormalization \n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IlOJ_ymiZ1m",
        "outputId": "021cb788-e6f0-480f-9e88-d8ace3a09e51"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ag5jzu2ibA1"
      },
      "source": [
        "Data = pd.read_pickle('/content/drive/MyDrive/Pattern_Recognition /Data.pickle')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8XPUlEGibE0"
      },
      "source": [
        "Data['creditLimit'] = StandardScaler().fit_transform(np.array(Data['creditLimit']).reshape(-1, 1))\n",
        "Data['availableMoney'] = StandardScaler().fit_transform(np.array(Data['availableMoney']).reshape(-1, 1))\n",
        "Data['transactionAmount'] = StandardScaler().fit_transform(np.array(Data['transactionAmount']).reshape(-1, 1))\n",
        "Data['currentBalance'] = StandardScaler().fit_transform(np.array(Data['currentBalance']).reshape(-1, 1))\n",
        "Data['Difference_open'] = StandardScaler().fit_transform(np.array(Data['Difference_open']).reshape(-1, 1))\n",
        "Data['Difference_exp'] = StandardScaler().fit_transform(np.array(Data['Difference_exp']).reshape(-1, 1))\n",
        "Data['Difference_address'] = StandardScaler().fit_transform(np.array(Data['Difference_address']).reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "j_LYg13YibIe",
        "outputId": "ae7d2af5-2925-4675-ab0e-4ca1c8b9822e"
      },
      "source": [
        "Data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>creditLimit</th>\n",
              "      <th>availableMoney</th>\n",
              "      <th>transactionAmount</th>\n",
              "      <th>currentBalance</th>\n",
              "      <th>cardPresent</th>\n",
              "      <th>expirationDateKeyInMatch</th>\n",
              "      <th>isCVVcorrect</th>\n",
              "      <th>isCountrySame</th>\n",
              "      <th>posEntryMode_05</th>\n",
              "      <th>posEntryMode_09</th>\n",
              "      <th>posEntryMode_80</th>\n",
              "      <th>posEntryMode_90</th>\n",
              "      <th>posConditionCode_08</th>\n",
              "      <th>posConditionCode_99</th>\n",
              "      <th>merchantCategoryCode_auto</th>\n",
              "      <th>merchantCategoryCode_cable/phone</th>\n",
              "      <th>merchantCategoryCode_entertainment</th>\n",
              "      <th>merchantCategoryCode_fastfood</th>\n",
              "      <th>merchantCategoryCode_food</th>\n",
              "      <th>merchantCategoryCode_food_delivery</th>\n",
              "      <th>merchantCategoryCode_fuel</th>\n",
              "      <th>merchantCategoryCode_furniture</th>\n",
              "      <th>merchantCategoryCode_gym</th>\n",
              "      <th>merchantCategoryCode_health</th>\n",
              "      <th>merchantCategoryCode_hotels</th>\n",
              "      <th>merchantCategoryCode_mobileapps</th>\n",
              "      <th>merchantCategoryCode_online_gifts</th>\n",
              "      <th>merchantCategoryCode_online_retail</th>\n",
              "      <th>merchantCategoryCode_online_subscriptions</th>\n",
              "      <th>merchantCategoryCode_personal care</th>\n",
              "      <th>merchantCategoryCode_rideshare</th>\n",
              "      <th>merchantCategoryCode_subscriptions</th>\n",
              "      <th>transactionType_PURCHASE</th>\n",
              "      <th>transactionType_REVERSAL</th>\n",
              "      <th>Difference_open</th>\n",
              "      <th>Difference_exp</th>\n",
              "      <th>Difference_address</th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.00000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>776668.000000</td>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>7.766680e+05</td>\n",
              "      <td>776668.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.045636e-14</td>\n",
              "      <td>4.170789e-15</td>\n",
              "      <td>4.509692e-16</td>\n",
              "      <td>1.910979e-14</td>\n",
              "      <td>0.448723</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.991077</td>\n",
              "      <td>0.999914</td>\n",
              "      <td>0.402630</td>\n",
              "      <td>0.302286</td>\n",
              "      <td>0.019557</td>\n",
              "      <td>0.025026</td>\n",
              "      <td>0.190523</td>\n",
              "      <td>0.009548</td>\n",
              "      <td>0.027483</td>\n",
              "      <td>0.001738</td>\n",
              "      <td>0.101908</td>\n",
              "      <td>0.142641</td>\n",
              "      <td>0.095899</td>\n",
              "      <td>0.007725</td>\n",
              "      <td>0.030426</td>\n",
              "      <td>0.00946</td>\n",
              "      <td>0.002811</td>\n",
              "      <td>0.024275</td>\n",
              "      <td>0.043351</td>\n",
              "      <td>0.019039</td>\n",
              "      <td>0.084237</td>\n",
              "      <td>0.257098</td>\n",
              "      <td>0.014070</td>\n",
              "      <td>0.024111</td>\n",
              "      <td>0.065034</td>\n",
              "      <td>0.029137</td>\n",
              "      <td>0.948457</td>\n",
              "      <td>0.025831</td>\n",
              "      <td>-6.669476e-15</td>\n",
              "      <td>5.942918e-17</td>\n",
              "      <td>-5.827390e-15</td>\n",
              "      <td>0.015407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>0.497364</td>\n",
              "      <td>0.036322</td>\n",
              "      <td>0.094038</td>\n",
              "      <td>0.009288</td>\n",
              "      <td>0.490428</td>\n",
              "      <td>0.459249</td>\n",
              "      <td>0.138471</td>\n",
              "      <td>0.156205</td>\n",
              "      <td>0.392714</td>\n",
              "      <td>0.097249</td>\n",
              "      <td>0.163486</td>\n",
              "      <td>0.041655</td>\n",
              "      <td>0.302528</td>\n",
              "      <td>0.349707</td>\n",
              "      <td>0.294453</td>\n",
              "      <td>0.087554</td>\n",
              "      <td>0.171757</td>\n",
              "      <td>0.09680</td>\n",
              "      <td>0.052942</td>\n",
              "      <td>0.153903</td>\n",
              "      <td>0.203645</td>\n",
              "      <td>0.136662</td>\n",
              "      <td>0.277743</td>\n",
              "      <td>0.437034</td>\n",
              "      <td>0.117781</td>\n",
              "      <td>0.153393</td>\n",
              "      <td>0.246586</td>\n",
              "      <td>0.168191</td>\n",
              "      <td>0.221103</td>\n",
              "      <td>0.158631</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>0.123164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-9.032733e-01</td>\n",
              "      <td>-8.173843e-01</td>\n",
              "      <td>-9.274101e-01</td>\n",
              "      <td>-6.982235e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.265281e+00</td>\n",
              "      <td>-1.858607e+00</td>\n",
              "      <td>-6.754237e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-4.948901e-01</td>\n",
              "      <td>-5.826284e-01</td>\n",
              "      <td>-6.995790e-01</td>\n",
              "      <td>-5.913691e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.503185e-01</td>\n",
              "      <td>-8.635414e-01</td>\n",
              "      <td>-6.332574e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-2.799516e-01</td>\n",
              "      <td>-3.452114e-01</td>\n",
              "      <td>-3.323410e-01</td>\n",
              "      <td>-3.185391e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.919486e-01</td>\n",
              "      <td>-2.626290e-03</td>\n",
              "      <td>-4.344731e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.648639e-01</td>\n",
              "      <td>1.411382e-01</td>\n",
              "      <td>3.691622e-01</td>\n",
              "      <td>1.208752e-01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.230144e-01</td>\n",
              "      <td>8.645123e-01</td>\n",
              "      <td>2.462124e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.374003e+00</td>\n",
              "      <td>4.930578e+00</td>\n",
              "      <td>1.269195e+01</td>\n",
              "      <td>6.657671e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.305375e+01</td>\n",
              "      <td>1.845057e+00</td>\n",
              "      <td>1.436893e+01</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        creditLimit  availableMoney  ...  Difference_address        isFraud\n",
              "count  7.766680e+05    7.766680e+05  ...        7.766680e+05  776668.000000\n",
              "mean   7.045636e-14    4.170789e-15  ...       -5.827390e-15       0.015407\n",
              "std    1.000001e+00    1.000001e+00  ...        1.000001e+00       0.123164\n",
              "min   -9.032733e-01   -8.173843e-01  ...       -6.754237e-01       0.000000\n",
              "25%   -4.948901e-01   -5.826284e-01  ...       -6.332574e-01       0.000000\n",
              "50%   -2.799516e-01   -3.452114e-01  ...       -4.344731e-01       0.000000\n",
              "75%    3.648639e-01    1.411382e-01  ...        2.462124e-01       0.000000\n",
              "max    3.374003e+00    4.930578e+00  ...        1.436893e+01       1.000000\n",
              "\n",
              "[8 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Bg2IhgibL6"
      },
      "source": [
        "X= Data.drop(['isFraud'], axis=1)\n",
        "y= Data['isFraud']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3nsXNt5ibPh"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOTFJSO6ibVB",
        "outputId": "49497447-b4ca-4ea6-bf89-9d1cb37e21a5"
      },
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "  \n",
        "# import SMOTE module from imblearn library\n",
        "# pip install imblearn (if you don't have imblearn in your system)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 2)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "  \n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
        "  \n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before OverSampling, counts of label '1': 9541\n",
            "Before OverSampling, counts of label '0': 611793 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "After OverSampling, the shape of train_X: (1223586, 37)\n",
            "After OverSampling, the shape of train_y: (1223586,) \n",
            "\n",
            "After OverSampling, counts of label '1': 611793\n",
            "After OverSampling, counts of label '0': 611793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LbURwZxibYx",
        "outputId": "4dfe90a1-53df-4c8d-ec8d-43ef1eb98373"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(100,input_dim=37,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 100)               3800      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 3,901\n",
            "Trainable params: 3,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLerbjLWibcK",
        "outputId": "3fb15fb0-2d36-42d3-c872-4f0db1736733"
      },
      "source": [
        "model.fit(X_train_res,y_train_res , validation_data=( X_test, y_test),batch_size=32,epochs=50,verbose=1)\n",
        "prediction=model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "38238/38238 [==============================] - 58s 2ms/step - loss: 0.5341 - accuracy: 0.7274 - val_loss: 0.4526 - val_accuracy: 0.7792\n",
            "Epoch 2/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.4537 - accuracy: 0.7815 - val_loss: 0.4211 - val_accuracy: 0.7940\n",
            "Epoch 3/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.4227 - accuracy: 0.7991 - val_loss: 0.4347 - val_accuracy: 0.7795\n",
            "Epoch 4/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.4098 - accuracy: 0.8057 - val_loss: 0.4660 - val_accuracy: 0.7644\n",
            "Epoch 5/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.4021 - accuracy: 0.8097 - val_loss: 0.4606 - val_accuracy: 0.7696\n",
            "Epoch 6/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3954 - accuracy: 0.8137 - val_loss: 0.3646 - val_accuracy: 0.8188\n",
            "Epoch 7/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3913 - accuracy: 0.8158 - val_loss: 0.3876 - val_accuracy: 0.8013\n",
            "Epoch 8/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3885 - accuracy: 0.8172 - val_loss: 0.4025 - val_accuracy: 0.7967\n",
            "Epoch 9/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3856 - accuracy: 0.8185 - val_loss: 0.4155 - val_accuracy: 0.7913\n",
            "Epoch 10/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3831 - accuracy: 0.8203 - val_loss: 0.3801 - val_accuracy: 0.8094\n",
            "Epoch 11/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3812 - accuracy: 0.8210 - val_loss: 0.4052 - val_accuracy: 0.7941\n",
            "Epoch 12/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3797 - accuracy: 0.8218 - val_loss: 0.5571 - val_accuracy: 0.7089\n",
            "Epoch 13/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3777 - accuracy: 0.8231 - val_loss: 0.3843 - val_accuracy: 0.8089\n",
            "Epoch 14/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3764 - accuracy: 0.8239 - val_loss: 0.4212 - val_accuracy: 0.7934\n",
            "Epoch 15/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3746 - accuracy: 0.8247 - val_loss: 0.4877 - val_accuracy: 0.7470\n",
            "Epoch 16/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3742 - accuracy: 0.8253 - val_loss: 0.4145 - val_accuracy: 0.7900\n",
            "Epoch 17/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3726 - accuracy: 0.8262 - val_loss: 0.4971 - val_accuracy: 0.7514\n",
            "Epoch 18/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3722 - accuracy: 0.8260 - val_loss: 0.3540 - val_accuracy: 0.8242\n",
            "Epoch 19/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3718 - accuracy: 0.8264 - val_loss: 0.4287 - val_accuracy: 0.7814\n",
            "Epoch 20/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3704 - accuracy: 0.8274 - val_loss: 0.4063 - val_accuracy: 0.7956\n",
            "Epoch 21/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3705 - accuracy: 0.8271 - val_loss: 0.4804 - val_accuracy: 0.7536\n",
            "Epoch 22/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3697 - accuracy: 0.8278 - val_loss: 0.3183 - val_accuracy: 0.8474\n",
            "Epoch 23/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3685 - accuracy: 0.8281 - val_loss: 0.2844 - val_accuracy: 0.8682\n",
            "Epoch 24/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3686 - accuracy: 0.8280 - val_loss: 0.3752 - val_accuracy: 0.8119\n",
            "Epoch 25/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3677 - accuracy: 0.8287 - val_loss: 0.4181 - val_accuracy: 0.7868\n",
            "Epoch 26/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3668 - accuracy: 0.8286 - val_loss: 0.4605 - val_accuracy: 0.7658\n",
            "Epoch 27/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3675 - accuracy: 0.8292 - val_loss: 0.2908 - val_accuracy: 0.8642\n",
            "Epoch 28/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3671 - accuracy: 0.8287 - val_loss: 0.4121 - val_accuracy: 0.7982\n",
            "Epoch 29/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3664 - accuracy: 0.8292 - val_loss: 0.3860 - val_accuracy: 0.8076\n",
            "Epoch 30/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3655 - accuracy: 0.8304 - val_loss: 0.4171 - val_accuracy: 0.7883\n",
            "Epoch 31/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3648 - accuracy: 0.8304 - val_loss: 0.4055 - val_accuracy: 0.7923\n",
            "Epoch 32/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3640 - accuracy: 0.8309 - val_loss: 0.4262 - val_accuracy: 0.7861\n",
            "Epoch 33/50\n",
            "38238/38238 [==============================] - 56s 1ms/step - loss: 0.3645 - accuracy: 0.8308 - val_loss: 0.4663 - val_accuracy: 0.7596\n",
            "Epoch 34/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3642 - accuracy: 0.8304 - val_loss: 0.4248 - val_accuracy: 0.7912\n",
            "Epoch 35/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3641 - accuracy: 0.8307 - val_loss: 0.3479 - val_accuracy: 0.8312\n",
            "Epoch 36/50\n",
            "38238/38238 [==============================] - 57s 2ms/step - loss: 0.3630 - accuracy: 0.8316 - val_loss: 0.3256 - val_accuracy: 0.8453\n",
            "Epoch 37/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3634 - accuracy: 0.8312 - val_loss: 0.4500 - val_accuracy: 0.7730\n",
            "Epoch 38/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3628 - accuracy: 0.8315 - val_loss: 0.3752 - val_accuracy: 0.8193\n",
            "Epoch 39/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3620 - accuracy: 0.8315 - val_loss: 0.4573 - val_accuracy: 0.7754\n",
            "Epoch 40/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3616 - accuracy: 0.8320 - val_loss: 0.3282 - val_accuracy: 0.8432\n",
            "Epoch 41/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3621 - accuracy: 0.8315 - val_loss: 0.3238 - val_accuracy: 0.8477\n",
            "Epoch 42/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3615 - accuracy: 0.8322 - val_loss: 0.2769 - val_accuracy: 0.8726\n",
            "Epoch 43/50\n",
            "38238/38238 [==============================] - 58s 2ms/step - loss: 0.3609 - accuracy: 0.8317 - val_loss: 0.3226 - val_accuracy: 0.8445\n",
            "Epoch 44/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3605 - accuracy: 0.8330 - val_loss: 0.3695 - val_accuracy: 0.8173\n",
            "Epoch 45/50\n",
            "38238/38238 [==============================] - 57s 1ms/step - loss: 0.3612 - accuracy: 0.8320 - val_loss: 0.3774 - val_accuracy: 0.8121\n",
            "Epoch 46/50\n",
            "38238/38238 [==============================] - 58s 2ms/step - loss: 0.3605 - accuracy: 0.8329 - val_loss: 0.3713 - val_accuracy: 0.8179\n",
            "Epoch 47/50\n",
            "38238/38238 [==============================] - 58s 2ms/step - loss: 0.3610 - accuracy: 0.8319 - val_loss: 0.3421 - val_accuracy: 0.8318\n",
            "Epoch 48/50\n",
            "38238/38238 [==============================] - 58s 2ms/step - loss: 0.3602 - accuracy: 0.8333 - val_loss: 0.2785 - val_accuracy: 0.8756\n",
            "Epoch 49/50\n",
            "38238/38238 [==============================] - 58s 2ms/step - loss: 0.3604 - accuracy: 0.8327 - val_loss: 0.4286 - val_accuracy: 0.7793\n",
            "Epoch 50/50\n",
            "38238/38238 [==============================] - 57s 2ms/step - loss: 0.3596 - accuracy: 0.8331 - val_loss: 0.4454 - val_accuracy: 0.7742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssxPvS4aibjA"
      },
      "source": [
        "lr_auc = roc_auc_score(y_test, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCmgk1JRibmj",
        "outputId": "62f2a7bf-b9ac-42c3-c55f-2081f03369de"
      },
      "source": [
        "lr_auc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.737484328156097"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "KL3tXaS3ibqM",
        "outputId": "f8c0d083-417a-4955-8354-864b719a6f16"
      },
      "source": [
        "# calculate roc curves\n",
        "fpr, tpr, thresholds = roc_curve(y_test, prediction)\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
        "pyplot.plot(fpr, tpr, marker='.', label='Model')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv351Xqdibt4",
        "outputId": "bfe0d67b-168a-40c3-b413-b28d1063ea7a"
      },
      "source": [
        "# calculate roc curves\n",
        "fpr, tpr, thresholds = roc_curve(y_test, prediction)\n",
        "# get the best threshold\n",
        "J = tpr - fpr\n",
        "ix = argmax(J)\n",
        "best_thresh = thresholds[ix]\n",
        "print('Best Threshold=%f' % (best_thresh))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Threshold=0.445583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzi_fDauibxM"
      },
      "source": [
        "ynn=(prediction>best_thresh).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU-MrJUwLym6",
        "outputId": "e53089bc-9148-4a36-f0d5-0680373812d8"
      },
      "source": [
        "print(classification_report(y_test, ynn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.74      0.85    152909\n",
            "           1       0.04      0.60      0.07      2425\n",
            "\n",
            "    accuracy                           0.74    155334\n",
            "   macro avg       0.51      0.67      0.46    155334\n",
            "weighted avg       0.98      0.74      0.84    155334\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}